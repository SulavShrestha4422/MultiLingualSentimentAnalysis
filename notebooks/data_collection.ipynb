{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a4f284b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Topic Definition ---\n",
      "EN: Quantum Computing\n",
      "DE: Quantencomputer\n",
      "HI: ‡§ï‡•ç‡§µ‡§æ‡§Ç‡§ü‡§Æ ‡§ï‡§Ç‡§™‡•ç‡§Ø‡•Ç‡§ü‡§ø‡§Ç‡§ó\n",
      "AR: ÿßŸÑÿ≠Ÿàÿ≥ÿ®ÿ© ÿßŸÑŸÉŸÖŸàŸÖŸäÿ©\n",
      "Initializing Reddit API connection...\n",
      "Connection successful. Starting data collection.\n",
      "\n",
      "--- Collecting EN Data ---\n",
      "‚úÖ Collected ~500 posts from r/artificial.\n",
      "‚úÖ Collected ~500 posts from r/tech.\n",
      "‚úÖ Collected ~500 posts from r/science.\n",
      "‚úÖ Collected ~500 posts via search: 'Quantum Computing...'.\n",
      "\n",
      "--- Collecting DE Data ---\n",
      "‚úÖ Collected ~500 posts from r/de.\n",
      "‚úÖ Collected ~500 posts from r/wissenschaft.\n",
      "‚ö†Ô∏è Could not fetch r/technik: received 403 HTTP response\n",
      "‚úÖ Collected ~500 posts via search: 'Quantencomputer...'.\n",
      "\n",
      "--- Collecting HI Data ---\n",
      "‚úÖ Collected ~500 posts from r/india.\n",
      "‚úÖ Collected ~500 posts from r/tech.\n",
      "‚úÖ Collected ~500 posts from r/scienceindia.\n",
      "‚úÖ Collected ~500 posts via search: '‡§ï‡•ç‡§µ‡§æ‡§Ç‡§ü‡§Æ ‡§ï‡§Ç‡§™‡•ç‡§Ø‡•Ç‡§ü‡§ø‡§Ç‡§ó...'.\n",
      "\n",
      "--- Collecting AR Data ---\n",
      "‚úÖ Collected ~500 posts from r/arabs.\n",
      "‚úÖ Collected ~500 posts from r/egypt.\n",
      "‚úÖ Collected ~500 posts from r/saudiarabia.\n",
      "‚úÖ Collected ~500 posts via search: 'ÿßŸÑÿ≠Ÿàÿ≥ÿ®ÿ© ÿßŸÑŸÉŸÖŸàŸÖŸäÿ©...'.\n",
      "\n",
      "==================================================\n",
      "             üöÄ COLLECTION COMPLETE üöÄ\n",
      "==================================================\n",
      "Total Collected Records: 2995\n",
      "Data saved to: c:\\Users\\Sulav\\OneDrive\\Desktop\\MultiLingualSentimentAnalysis\\data\\raw_multilingual_data.csv\n",
      "\n",
      "üî• Ready for Step 3: Data Preprocessing (Cleaning and Language Detection)\n"
     ]
    }
   ],
   "source": [
    "# --- 1. SETUP AND CONFIGURATION ---\n",
    "\n",
    "# Install necessary libraries if you haven't already (uncomment the line below if needed)\n",
    "# !pip install praw pandas\n",
    "\n",
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "# --- PRAW CONFIGURATION (YOUR CREDENTIALS) ---\n",
    "CLIENT_ID = \"kDC0_tOIbEZUDh6LHJDEUA\"\n",
    "CLIENT_SECRET = \"FA-jWQz9oFJG59z5POp4OWD6mb_6aQ\"\n",
    "USER_AGENT = \"multilingual-sentiment-project\"\n",
    "\n",
    "# --- FILE & PATH CONFIGURATION ---\n",
    "DATA_DIR = '../data/' \n",
    "OUTPUT_FILE = DATA_DIR + 'raw_multilingual_data.csv'\n",
    "os.makedirs(DATA_DIR, exist_ok=True) \n",
    "\n",
    "# --- DATA COLLECTION PARAMETERS ---\n",
    "COLLECTION_LIMIT = 500  # Max number of submissions to collect per search/subreddit\n",
    "\n",
    "\n",
    "def define_user_topics():\n",
    "    \"\"\"\n",
    "    Defines the central topic across all four languages using hardcoded values \n",
    "    for non-interactive execution.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"\\n--- Topic Definition ---\")\n",
    "    \n",
    "    # Replace these hardcoded strings if you want to change the topic.\n",
    "    # To use interactive input, uncomment the 'input()' lines and comment out the hardcoded lines.\n",
    "    \n",
    "    topic_en = \"Quantum Computing\"  # English topic\n",
    "    # topic_en = input(\"Enter Topic (English): \")\n",
    "    \n",
    "    topic_de = \"Quantencomputer\"   # German topic\n",
    "    # topic_de = input(\"Enter Topic (German): \")\n",
    "    \n",
    "    topic_hi = \"‡§ï‡•ç‡§µ‡§æ‡§Ç‡§ü‡§Æ ‡§ï‡§Ç‡§™‡•ç‡§Ø‡•Ç‡§ü‡§ø‡§Ç‡§ó\" # Hindi topic\n",
    "    # topic_hi = input(\"Enter Topic (Hindi): \")\n",
    "    \n",
    "    topic_ar = \"ÿßŸÑÿ≠Ÿàÿ≥ÿ®ÿ© ÿßŸÑŸÉŸÖŸàŸÖŸäÿ©\"  # Arabic topic\n",
    "    # topic_ar = input(\"Enter Topic (Arabic): \")\n",
    "    \n",
    "    if not all([topic_en, topic_de, topic_hi, topic_ar]):\n",
    "        # This will only happen if using input() and the user enters nothing\n",
    "        raise ValueError(\"All four topic inputs must be provided.\")\n",
    "    \n",
    "    print(f\"EN: {topic_en}\\nDE: {topic_de}\\nHI: {topic_hi}\\nAR: {topic_ar}\")\n",
    "    \n",
    "    # Base Subreddits (kept consistent as they are relevant general/regional subreddits)\n",
    "    # The user's topic is applied via the 'queries' list.\n",
    "    return {\n",
    "        'en': {\n",
    "            'subreddits': ['artificial', 'tech', 'science'],\n",
    "            'queries': [topic_en]\n",
    "        },\n",
    "        'de': {\n",
    "            'subreddits': ['de', 'wissenschaft', 'technik'],\n",
    "            'queries': [topic_de]\n",
    "        },\n",
    "        'hi': {\n",
    "            'subreddits': ['india', 'tech', 'scienceindia'],\n",
    "            'queries': [topic_hi]\n",
    "        },\n",
    "        'ar': {\n",
    "            'subreddits': ['arabs', 'egypt', 'saudiarabia'],\n",
    "            'queries': [topic_ar]\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Execute topic definition\n",
    "REDDIT_SOURCES = define_user_topics()\n",
    "\n",
    "\n",
    "# --- 2. DATA COLLECTION FUNCTION ---\n",
    "\n",
    "def collect_reddit_data(client_id, client_secret, user_agent, sources):\n",
    "    \"\"\"Authenticates PRAW and collects data using the user-defined sources.\"\"\"\n",
    "    print(\"Initializing Reddit API connection...\")\n",
    "    try:\n",
    "        reddit = praw.Reddit(\n",
    "            client_id=client_id,\n",
    "            client_secret=client_secret,\n",
    "            user_agent=user_agent,\n",
    "        )\n",
    "        reddit.read_only = True\n",
    "        print(\"Connection successful. Starting data collection.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error initializing PRAW: {e}. Check your credentials.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    all_data = []\n",
    "    \n",
    "    for lang_code, data_sources in sources.items():\n",
    "        print(f\"\\n--- Collecting {lang_code.upper()} Data ---\")\n",
    "\n",
    "        # --- 1. Collect from Subreddits (TOP posts) ---\n",
    "        for sub_name in data_sources['subreddits']:\n",
    "            try:\n",
    "                subreddit = reddit.subreddit(sub_name)\n",
    "                for submission in subreddit.top(time_filter=\"year\", limit=COLLECTION_LIMIT):\n",
    "                    full_text = f\"{submission.title} {submission.selftext}\"\n",
    "                    \n",
    "                    if full_text.strip():\n",
    "                        all_data.append({\n",
    "                            'text': full_text.strip(),\n",
    "                            'language_guess': lang_code,\n",
    "                            'source_type': 'Reddit_Subreddit',\n",
    "                            'source_name': sub_name,\n",
    "                            'raw_timestamp': submission.created_utc\n",
    "                        })\n",
    "                print(f\"‚úÖ Collected ~{COLLECTION_LIMIT} posts from r/{sub_name}.\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Could not fetch r/{sub_name}: {e}\")\n",
    "                time.sleep(3) \n",
    "\n",
    "        # --- 2. Collect from General Reddit Search (using user-defined topic) ---\n",
    "        for query in data_sources['queries']:\n",
    "            try:\n",
    "                for submission in reddit.subreddit('all').search(\n",
    "                    query, sort='relevance', limit=COLLECTION_LIMIT\n",
    "                ):\n",
    "                    full_text = f\"{submission.title} {submission.selftext}\"\n",
    "\n",
    "                    if full_text.strip():\n",
    "                        all_data.append({\n",
    "                            'text': full_text.strip(),\n",
    "                            'language_guess': lang_code,\n",
    "                            'source_type': 'Reddit_Search',\n",
    "                            'source_name': f\"Search: {query}\",\n",
    "                            'raw_timestamp': submission.created_utc\n",
    "                        })\n",
    "                print(f\"‚úÖ Collected ~{COLLECTION_LIMIT} posts via search: '{query[:20]}...'.\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Could not execute search '{query}': {e}\")\n",
    "                time.sleep(3)\n",
    "                \n",
    "        # Pause to respect API rate limits\n",
    "        time.sleep(random.randint(5, 10))\n",
    "\n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "\n",
    "# --- 3. EXECUTION AND FINAL SAVE ---\n",
    "\n",
    "# Execute the combined collection for EN, DE, HI, and AR\n",
    "final_df = collect_reddit_data(CLIENT_ID, CLIENT_SECRET, USER_AGENT, REDDIT_SOURCES)\n",
    "\n",
    "# Save the final data to the specified 'data' folder\n",
    "final_df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8')\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"             üöÄ COLLECTION COMPLETE üöÄ\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total Collected Records: {len(final_df)}\")\n",
    "print(f\"Data saved to: {os.path.abspath(OUTPUT_FILE)}\")\n",
    "print(\"\\nüî• Ready for Step 3: Data Preprocessing (Cleaning and Language Detection)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multi-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
